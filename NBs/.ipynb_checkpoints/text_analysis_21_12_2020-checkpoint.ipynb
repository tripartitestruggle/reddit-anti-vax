{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text analysis\n",
    "## r/conspiracy comments containing \"vaccine\"\n",
    "**Group members**\n",
    "* Maheep T\n",
    "* Sanchi S\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b51b90846e6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from pandas.compat.numpy import (\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mnp_version_under1p17\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_np_version_under1p17\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mnp_version_under1p18\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_np_version_under1p18\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mPY38\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# array-like\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mAnyArrayLike\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"AnyArrayLike\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ExtensionArray\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Index\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Series\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[0mArrayLike\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTypeVar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ArrayLike\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ExtensionArray\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'ndarray'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from dfply import *\n",
    "import preprocessor as p\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from kwic import keywords_in_context\n",
    "from operator import itemgetter\n",
    "from datetime import datetime as dt\n",
    "from keyness import log_likelihood\n",
    "\n",
    "# download\n",
    "# nltk.download\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "base_dir = 'C:\\\\Users\\\\Maheep\\\\Documents\\\\TUM\\\\WiSe2020\\\\CSS\\\\code' #import os; os.getcwd()\n",
    "\n",
    "filename = base_dir + \"\\\\\" + \"comments_vaccine_praw.csv\" # comments_vaccine/_praw.csv\n",
    "df_com = pd.read_csv(filename)\n",
    "df_com = df_com.rename({'Submission ID': 's_id', 'Comment ID': 'c_id', 'Body': 'text', 'Publish Date': 'date', 'Author': 'user', 'Score': 'score', 'Permalink': 'link'}, axis=1)\n",
    "\n",
    "filename = base_dir + \"\\\\\" + \"submissions_vaccine.csv\"\n",
    "df_sub = pd.read_csv(filename)\n",
    "df_sub = df_sub.rename({'Post ID': 's_id', 'Title': 'text', 'Publish Date': 'date', 'Score': 'score', 'Permalink': 'link'}, axis=1)\n",
    "\n",
    "filename = base_dir + \"\\\\\" + \"reddit_bots.csv\"\n",
    "df_bots = pd.read_csv(filename)\n",
    "list_bots = df_bots['Name'].tolist() ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# out_file for temp output\n",
    "out_file = base_dir + \"\\\\\" + \"out_file.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Use pushshift to query \"vaccine\" submissions (reddit posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(df_sub.shape[0], \" rows in submission data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Use PRAW to collect comments by iterating over all \"vaccine\" submissions\n",
    "([praw docs](https://praw.readthedocs.io/en/latest/tutorials/comments.html))\n",
    "    * pushshift comment data has a lot of missing values ([Gaffney, Matias 2018](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200162) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(df_com.shape[0], \"rows in comment data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clean-up and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Remove comments -\n",
    "  * where user and text were [deleted]\n",
    "  * where user is a  bot (list of bots from [Klein et. al. 2019](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0225098) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_com['date'] =  df_com['date'].apply(lambda x: dt.strptime(x,'%Y-%m-%d %H:%M:%S')) # convert str to datetime\n",
    "df_com_pp = df_com.copy() # copy to pre-process dataframe\n",
    "df_com_pp = df_com_pp >> mask(~((X.user == '[deleted]') & (X.text == '[deleted]'))) # >> summarize(count=n(X.c_id))\n",
    "df_com_pp = df_com_pp[~df_com_pp['user'].isin(list_bots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Clean - removes URLs, Hashtags, Mentions, Reserved words (RT, FAV), Emojis, Smileys ([Source](https://pypi.org/project/tweet-preprocessor/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def func_clean(text):\n",
    "    text = re.sub(r'[\\r|\\n|\\r\\n]+', \" \",text) # remove extra newlines\n",
    "    text = p.clean(text) # from preprocessor lib\n",
    "    return text\n",
    "    \n",
    "df_com_pp['text'] = df_com_pp['text'].apply(func_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Expanding contractions ([Source](https://github.com/Apress/text-analytics-w-python-2e/blob/master/Ch03%20-%20Processing%20and%20Understanding%20Text/Ch03a%20-%20Text%20Wrangling.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "df_com_pp['text'] = df_com_pp['text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Tokenise <= lowercase + remove punctuation ([Source](https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_com_pp['text'] = df_com_pp['text'].apply(lambda x: x.lower()) # lowercase\n",
    "df_com_pp['text'] = df_com_pp['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation))) # remove punctuation\n",
    "\n",
    "tt = TweetTokenizer() # does better than default word_tokenizer\n",
    "df_com_pp['text'] = df_com_pp['text'].apply(tt.tokenize) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Fix repetition in words e.g. \"Coooolll!\" => \"Cool!\" ([Source](https://github.com/Apress/text-analytics-w-python-2e/blob/master/Ch03%20-%20Processing%20and%20Understanding%20Text/Ch03a%20-%20Text%20Wrangling.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "df_com_pp['text'] = df_com_pp['text'].apply(remove_repeated_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "def func_filter_stopwords(l):\n",
    "    return list([w for w in l if not w in stop_words])\n",
    "df_com_pp['text'] = df_com_pp['text'].apply(func_filter_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Stemming (not lemmatising as it needs the whole sentence context to be effective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "df_com_pp['text'] = df_com_pp['text'].apply(lambda x: [ps.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Weekly comment frequency on \"vaccine\" posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_com_pp['week'] = pd.to_datetime(df_com_pp['date']).dt.isocalendar().week\n",
    "df_new = df_com_pp.groupby('week').size()\n",
    "\n",
    "fig = plt.figure(figsize=[10,8])\n",
    "ax = plt.subplot()\n",
    "ax = df_new.plot.bar()\n",
    "ax.set_xlabel('Week in 2020')\n",
    "ax.set_ylabel('Count of comments on \"vaccine\" posts on r/conspiracy')\n",
    "ax.set_title('Weekly comment frequency')\n",
    "# #plt.grid()\n",
    "plt.show()\n",
    "# plt.cla()\n",
    "# plt.clf()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Weekly comment frequency of comments mentioning \"bill gate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "def func_gates(x):\n",
    "    return True if (('bill' in x) & ('gate' in x)) else False\n",
    "df_com_pp['gates'] = df_com_pp['text'].apply(lambda x: func_gates(x))\n",
    "df_com_pp >> filter_by(X.gates == True)\n",
    "df_new = df_com_pp.groupby('week').agg({'gates' : \"sum\"})\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches([10,10])\n",
    "#ax = df_new.plot.bar(y='gates')\n",
    "ax.bar(df_new.index,df_new['gates'].tolist())\n",
    "ax.set_xticks(df_new.index.tolist())\n",
    "ax.set_xlabel('Week in 2020')\n",
    "ax.set_ylabel('Count of \\'bill gate\\' mentions in comments on r/conspiracy')\n",
    "ax.set_title('\\'bill gate\\' frequency')\n",
    "# #plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Weekly comment frequency of comments mentioning \"side effect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def func_effect(x):\n",
    "    return True if (('side' in x) & ('effect' in x)) else False\n",
    "df_com_pp['effect'] = df_com_pp['text'].apply(lambda x: func_effect(x))\n",
    "df_new = df_com_pp.groupby('week').agg({'effect' : \"sum\"})\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches([10,10])\n",
    "#ax = df_new.plot.bar(y='gates')\n",
    "ax.bar(df_new.index,df_new['effect'].tolist())\n",
    "ax.set_xticks(df_new.index.tolist())\n",
    "ax.set_xlabel('Week in 2020')\n",
    "ax.set_ylabel('Count of \\'side effect\\' mentions in comments on r/conspiracy')\n",
    "ax.set_title('\\'side effect\\' frequency')\n",
    "# #plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Most important words/phrases ([Source](https://github.com/Apress/text-analytics-w-python-2e/blob/master/Ch06%20-%20Text%20Summarization%20and%20Topic%20Models/Ch06a%20-%20Text%20Keyphrase%20Extraction.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def compute_ngrams(sequence, n):\n",
    "    return list(\n",
    "            zip(*(sequence[index:] \n",
    "                     for index in range(n)))\n",
    "    )\n",
    "\n",
    "def get_top_ngrams(tokens, ngram_val=1, limit=5):\n",
    "\n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
    "    sorted_ngrams = [(' '.join(text), freq) \n",
    "                     for text, freq in sorted_ngrams]\n",
    "\n",
    "    return sorted_ngrams\n",
    "list_tokens_flattened = [token for text in df_com_pp['text'].tolist() for token in text]\n",
    "#get_top_ngrams(list_tokens_flattened,2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "get_top_ngrams(list_tokens_flattened,2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "get_top_ngrams(list_tokens_flattened,1,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* DFM ([Source](https://blog.koheiw.net/?p=468))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "print(\"Serialize tokens\")\n",
    "dic = corpora.Dictionary(df_com_pp['text'].tolist())\n",
    "\n",
    "print(\"Construct a document-feature matrix\")\n",
    "mx = [dic.doc2bow(tok) for tok in df_com_pp['text'].tolist()]\n",
    "\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Keyness (Sources [1](http://www.thegrammarlab.com/?p=193) [2](http://ucrel.lancs.ac.uk/llwizard.html))\n",
    "  * The log_likelihood() function will return a rank ordered list with the following data in each item: - the item - its log likelihood value - its frequency in the corpus - its frequency in the reference corpus ([Source](https://github.com/mikesuhan/keyness))\n",
    "  * ct.keyness() gives only effect sizes are reported (p values are arguably not particularly useful for keyness analyses). Keyness calculation options include \"log-ratio\", \"%diff\", and \"odds-ratio\". ([Source](https://github.com/kristopherkyle/corpus_toolkit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#cutoff_date = dt(2020,10,1,0,0,0)\n",
    "#cutoff_date = dt(2020,7,27,0,0,0) #Pfizer and BionTech start trials\n",
    "cutoff_date = dt(2020,12,2,0,0,0) #Pfizer and BionTech start trials\n",
    "\n",
    "df_temp = df_com_pp >> filter_by(X.date > cutoff_date)\n",
    "corpus = df_temp['text'].tolist()\n",
    "\n",
    "df_temp = df_com_pp >> filter_by(~(X.date > cutoff_date))\n",
    "reference_corpus = df_temp['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Cutoff date is \", cutoff_date)\n",
    "log_likelihood(corpus, reference_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from corpus_toolkit import corpus_tools as ct\n",
    "\n",
    "#First, generate frequency lists for each corpus\n",
    "corp1freq = ct.frequency(reference_corpus)\n",
    "corp2freq = ct.frequency(corpus)\n",
    "\n",
    "#then calculate Keyness\n",
    "corp_key = ct.keyness(corp1freq, corp2freq, effect = \"log-ratio\")\n",
    "ct.head(corp_key, hits = 10) #to display top hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "corp1freq['bihar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df_temp = df_com >> filter_by(X.date > cutoff_date)\n",
    "df_temp['text'] = df_temp['text'].str.lower()\n",
    "KEYWORDS = ['bihar']\n",
    "for i in range(0,df_temp.shape[0]):\n",
    "    TEXT = df_temp[i,2]\n",
    "    keywords_in_context(TEXT, KEYWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "## Parking lot\n",
    "  * text analysis with python vid [link](https://www.youtube.com/watch?v=ALGd_frv4rc&feature=youtu.be&ab_channel=J%C3%BCrgenPfeffer)\n",
    "  * SNA in python vids [link](https://www.youtube.com/playlist?list=PLW5N2mhH4QJqRKrHLWs052mhXyFAsQn-v) "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
